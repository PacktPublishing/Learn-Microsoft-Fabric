{"cells":[{"cell_type":"markdown","source":["### Dimension - City\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone and writes it as delta lake table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"92e2d78d-2087-4128-bfe3-f572fab9a8cc"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","table_name = 'dimension_city'\r\n","\r\n","dimension_city_schema = StructType([\r\n","    StructField('CityKey', IntegerType(), True), \r\n","    StructField('WWICityID', IntegerType(), True), \r\n","    StructField('City', StringType(), True), \r\n","    StructField('StateProvince', StringType(), True), \r\n","    StructField('Country', StringType(), True), \r\n","    StructField('Continent', StringType(), True), \r\n","    StructField('SalesTerritory', StringType(), True), \r\n","    StructField('Region', StringType(), True), \r\n","    StructField('Subregion', StringType(), True), \r\n","    StructField('Location', StringType(), True), \r\n","    StructField('LatestRecordedPopulation', LongType(), True), \r\n","    StructField('ValidFrom', TimestampType(), True), \r\n","    StructField('ValidTo', TimestampType(), True), \r\n","    StructField('LineageKey', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ab158a21-4602-43d6-bb23-0078db8e019f"},{"cell_type":"markdown","source":["### Dimension - Customer\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone and writes it as delta lake table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"479f598f-139e-4c24-92e8-9596a353f5f7"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","table_name = 'dimension_customer'\r\n","\r\n","dimension_customer_schema = StructType([\r\n","    StructField('CustomerKey', IntegerType(), True), \r\n","    StructField('WWICustomerID', IntegerType(), True), \r\n","    StructField('Customer', StringType(), True), \r\n","    StructField('BillToCustomer', StringType(), True), \r\n","    StructField('Category', StringType(), True), \r\n","    StructField('BuyingGroup', StringType(), True), \r\n","    StructField('PrimaryContact', StringType(), True), \r\n","    StructField('PostalCode', StringType(), True), \r\n","    StructField('ValidFrom', TimestampType(), True), \r\n","    StructField('ValidTo', TimestampType(), True), \r\n","    StructField('LineageKey', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5dfd5cfb-ba51-4266-8c35-afc6df94c86c"},{"cell_type":"markdown","source":["### Dimension - Date\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone and writes it as delta lake table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4290ad2e-ba5d-4033-955a-9187dfab52dc"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","table_name = 'dimension_date'\r\n","\r\n","dimension_date_schema = StructType([\r\n","    StructField('Date', TimestampType(), True), \r\n","    StructField('DayNumber', IntegerType(), True), \r\n","    StructField('Day', StringType(), True), \r\n","    StructField('Month', StringType(), True), \r\n","    StructField('ShortMonth', StringType(), True), \r\n","    StructField('CalendarMonthNumber', IntegerType(), True), \r\n","    StructField('CalendarMonthLabel', StringType(), True), \r\n","    StructField('CalendarYear', IntegerType(), True), \r\n","    StructField('CalendarYearLabel', StringType(), True), \r\n","    StructField('FiscalMonthNumber', IntegerType(), True), \r\n","    StructField('FiscalMonthLabel', StringType(), True), \r\n","    StructField('FiscalYear', IntegerType(), True), \r\n","    StructField('FiscalYearLabel', StringType(), True), \r\n","    StructField('ISOWeekNumber', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"df66a001-5d6f-457f-8e81-6bb738dd79f2"},{"cell_type":"markdown","source":["### Dimension - Employee\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone and writes it as delta lake table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ae4063de-17f0-45be-9e46-6a4e051086ab"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","table_name = 'dimension_employee'\r\n","\r\n","dimension_employee_schema = StructType([\r\n","    StructField('EmployeeKey', IntegerType(), True), \r\n","    StructField('WWIEmployeeID', IntegerType(), True), \r\n","    StructField('Employee', StringType(), True), \r\n","    StructField('PreferredName', StringType(), True), \r\n","    StructField('IsSalesperson', BooleanType(), True), \r\n","    StructField('Photo', StringType(), True), \r\n","    StructField('ValidFrom', TimestampType(), True), \r\n","    StructField('ValidTo', TimestampType(), True), \r\n","    StructField('LineageKey', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(dimension_employee_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"580c885c-eebb-473a-a865-338e9687f2ae"},{"cell_type":"markdown","source":["### Dimension - Stock Item\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone and writes it as delta lake table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6c88b899-4a5a-40ca-b746-c70c51751345"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","table_name = 'dimension_stock_item'\r\n","\r\n","dimension_stock_item_schema = StructType([\r\n","    StructField('StockItemKey', IntegerType(), True), \r\n","    StructField('WWIStockItemID', IntegerType(), True), \r\n","    StructField('StockItem', StringType(), True), \r\n","    StructField('Color', StringType(), True), \r\n","    StructField('SellingPackage', StringType(), True), \r\n","    StructField('BuyingPackage', StringType(), True), \r\n","    StructField('Brand', StringType(), True), \r\n","    StructField('Size', StringType(), True), \r\n","    StructField('LeadTimeDays', IntegerType(), True), \r\n","    StructField('QuantityPerOuter', IntegerType(), True), \r\n","    StructField('IsChillerStock', BooleanType(), True), \r\n","    StructField('Barcode', StringType(), True), \r\n","    StructField('TaxRate', DecimalType(18,3), True), \r\n","    StructField('UnitPrice', DecimalType(18,2), True), \r\n","    StructField('RecommendedRetailPrice', DecimalType(18,2), True), \r\n","    StructField('TypicalWeightPerUnit', DecimalType(18,3), True), \r\n","    StructField('Photo', StringType(), True), \r\n","    StructField('ValidFrom', TimestampType(), True), \r\n","    StructField('ValidTo', TimestampType(), True), \r\n","    StructField('LineageKey', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(dimension_stock_item_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"998b900c-3270-4aab-9474-4773f05873cd"},{"cell_type":"markdown","source":["### Fact - Sale\r\n","\r\n","This cell defines a variable and assigns the name of the table being loaded data for. Next, it defines schema of the data coming from set of _csv files_ for this specific table. This explicit definition of schema optimizes data load performance. Finally, it reads raw data from _csv files_ coming from bronze zone, add additional computed columns to it and writes it as delta lake table, partitioned by year and quarter."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6bc50327-0947-4aef-a00d-cedc33a1c17a"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","from pyspark.sql.functions import col, year, month, quarter\r\n","\r\n","table_name = 'fact_sale'\r\n","\r\n","fact_sale_schema = StructType([\r\n","    StructField('SaleKey', LongType(), True), \r\n","    StructField('CityKey', IntegerType(), True), \r\n","    StructField('CustomerKey', IntegerType(), True), \r\n","    StructField('BillToCustomerKey', IntegerType(), True), \r\n","    StructField('StockItemKey', IntegerType(), True), \r\n","    StructField('InvoiceDateKey', TimestampType(), True), \r\n","    StructField('DeliveryDateKey', TimestampType(), True), \r\n","    StructField('SalespersonKey', IntegerType(), True), \r\n","    StructField('WWIInvoiceID', IntegerType(), True), \r\n","    StructField('Description', StringType(), True), \r\n","    StructField('Package', StringType(), True), \r\n","    StructField('Quantity', IntegerType(), True), \r\n","    StructField('UnitPrice', DecimalType(18,2), True), \r\n","    StructField('TaxRate', DecimalType(18,3), True), \r\n","    StructField('TotalExcludingTax', DecimalType(29,2), True), \r\n","    StructField('TaxAmount', DecimalType(38,6), True), \r\n","    StructField('Profit', DecimalType(18,2), True), \r\n","    StructField('TotalIncludingTax', DecimalType(38,6), True), \r\n","    StructField('TotalDryItems', IntegerType(), True), \r\n","    StructField('TotalChillerItems', IntegerType(), True), \r\n","    StructField('LineageKey', IntegerType(), True)])\r\n","\r\n","df = spark.read.format(\"csv\").schema(fact_sale_schema).option(\"header\",\"true\").load('Files/wwi/full/' + table_name)\r\n","\r\n","df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\r\n","df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\r\n","df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\r\n","\r\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7481e33f-4ebc-4355-82a4-bee0f47823be"},{"cell_type":"code","source":["%%sql\r\n","SELECT Year, Quarter, Month, count(*)\r\n","FROM wwi_silver.fact_sale \r\n","GROUP BY Year, Quarter, Month\r\n","ORDER BY Year, Quarter, Month;"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"afb94a19-e840-4758-8ffe-3efca02e1947"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{"synapse_widget":{"token":"478b1410-0d8d-4571-9033-37be91d75231","state":{}}}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}